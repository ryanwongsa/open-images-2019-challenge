{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from dataloader import get_dataloader\n",
    "from preprocess import transformer, img_transform, reverse_img_transform\n",
    "from models import RetinaNet\n",
    "from loss import FocalLoss\n",
    "from train import Trainer, load_components\n",
    "from inference import InferenceTransform\n",
    "from evaluation import support_evaluate_model\n",
    "from utils import Visualiser\n",
    "from callbacks import Callback\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"gatletag/Local-Object-Detection-Tests\"\n",
    "experiment_name = \"local_test1\"\n",
    "\n",
    "dir_params = {\n",
    "    \"train_images_dir\": \"dataset/validation\",\n",
    "    \"train_bbox_dir\": \"data_info/valid/annotations/valid-anno.json\",\n",
    "    \"train_idx_to_id_dir\": \"data_info/valid/annotations/valid-idx_to_id.pkl\",\n",
    "\n",
    "    \"valid_images_dir\": \"dataset/validation\",\n",
    "    \"valid_bbox_dir\": \"data_info/valid/annotations/valid-anno.json\",\n",
    "    \"valid_idx_to_id_dir\": \"data_info/valid/annotations/valid-idx_to_id.pkl\",\n",
    "\n",
    "    \"clsids_to_idx_dir\": \"data_info/clsids_to_idx.pkl\",\n",
    "    \"clsids_to_names_dir\": \"data_info/clsids_to_names.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    # speed parameters\n",
    "    \"num_workers\": 8,\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # dataloader parameters\n",
    "    \"bs\": 16,\n",
    "    \"img_dim\": 512,\n",
    "\n",
    "    # anchor parameters\n",
    "    \"ratios\": [1/3, 1/2, 1, 2],\n",
    "    \"scales\": [0.25, 1, 2],\n",
    "\n",
    "    # network parameters\n",
    "    \"backbone\": \"resnet50\",\n",
    "    \"num_classes\": 501,\n",
    "    \"pretrained\": True,\n",
    "    \"freeze_bn\": True,\n",
    "\n",
    "    # loss parameters\n",
    "    \"alpha\": 0.25,\n",
    "    \"gamma\": 2.0,\n",
    "    \"IoU_bkgrd\":0.4,\n",
    "    \"IoU_pos\":0.5,\n",
    "    \"regress_factor\": [0.1, 0.1, 0.2, 0.2],\n",
    "\n",
    "    # optimizer parameters\n",
    "    \"lr\": 0.001,\n",
    "    \"min_lr\": 0.000001,\n",
    "    \"patience\": 100,\n",
    "    \"decay_factor\": 0.3,\n",
    "\n",
    "    # training parameters\n",
    "    \"epochs\": 2,\n",
    "    \"checkpoint_dir\": \"temp/final.pth\",\n",
    "    \"save_dir\": \"temp\",\n",
    "    \"fine_tune\": True,\n",
    "\n",
    "    # evaluation parameters\n",
    "    \"cls_thresh\":0.10, \n",
    "    \"overlap\":0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Class Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsids_to_names = pickle.load(open(dir_params[\"clsids_to_names_dir\"],'rb'))\n",
    "clsids_to_idx = pickle.load(open(dir_params[\"clsids_to_idx_dir\"],'rb'))\n",
    "idx_to_cls_ids = {v: k for k, v in clsids_to_idx.items()}\n",
    "idx_to_names = {k: clsids_to_names[v] for k, v in idx_to_cls_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": int(hyper_params[\"img_dim\"]*1.05), \"width\": int(hyper_params[\"img_dim\"]*1.05)}),\n",
    "        iaa.GammaContrast((0.9,1.1)),\n",
    "        iaa.Affine(rotate=(-5, 5), scale=(0.90, 1.10)),\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.00)),\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "train_transform_fn = transformer(train_seq, img_transform)\n",
    "\n",
    "train_dl = get_dataloader(\n",
    "        dir_params[\"train_images_dir\"], dir_params[\"train_bbox_dir\"], \n",
    "        dir_params[\"train_idx_to_id_dir\"], clsids_to_idx,\n",
    "        train_transform_fn, hyper_params[\"bs\"], True, hyper_params[\"num_workers\"], True\n",
    "    )\n",
    "\n",
    "valid_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "valid_transform_fn = transformer(valid_seq, img_transform)\n",
    "\n",
    "valid_dl = get_dataloader(\n",
    "        dir_params[\"valid_images_dir\"], \n",
    "        dir_params[\"valid_bbox_dir\"], \n",
    "        dir_params[\"valid_idx_to_id_dir\"], \n",
    "        clsids_to_idx, \n",
    "        valid_transform_fn, hyper_params[\"bs\"], False, hyper_params[\"num_workers\"], False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Loss Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(\n",
    "        hyper_params[\"alpha\"], \n",
    "        hyper_params[\"gamma\"], \n",
    "        hyper_params[\"IoU_bkgrd\"], \n",
    "        hyper_params[\"IoU_pos\"], \n",
    "        hyper_params[\"regress_factor\"], \n",
    "        hyper_params[\"device\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet = RetinaNet(\n",
    "        hyper_params[\"backbone\"], \n",
    "        hyper_params[\"num_classes\"],\n",
    "        hyper_params[\"ratios\"], \n",
    "        hyper_params[\"scales\"], \n",
    "        device=hyper_params[\"device\"], \n",
    "        pretrained = hyper_params[\"pretrained\"], \n",
    "        freeze_bn = hyper_params[\"freeze_bn\"],\n",
    "        prior=0.01, \n",
    "        feature_size=256, \n",
    "        pyramid_levels = [3, 4, 5, 6, 7],\n",
    "        criterion=criterion\n",
    "    )\n",
    "\n",
    "# TODO: Move this over to training file\n",
    "def set_parameter_requires_grad(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if (name.split('.')[0]) not in [\"fpn\", \"regressionModel\", \"classificationModel\"]:\n",
    "                param.requires_grad = False\n",
    "\n",
    "if hyper_params[\"fine_tune\"]==True:\n",
    "    set_parameter_requires_grad(retinanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Optimizer Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(retinanet.parameters(), lr=hyper_params[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "        mode='min', \n",
    "        factor=hyper_params['decay_factor'], \n",
    "        patience=hyper_params[\"patience\"], \n",
    "        verbose=True, \n",
    "        min_lr=hyper_params[\"min_lr\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Visualisation Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualiser(\n",
    "        hyper_params[\"num_classes\"],\n",
    "        idx_to_names,\n",
    "        reverse_img_transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Inference Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferenceTransform(\n",
    "        idx_to_names,\n",
    "        idx_to_cls_ids,\n",
    "        hyper_params[\"regress_factor\"]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: temp/final.pth\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Using Multiple GPUs\n",
      "LOD-79\n",
      "https://ui.neptune.ml/gatletag/Local-Object-Detection-Tests/e/LOD-79\n"
     ]
    }
   ],
   "source": [
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "load_components(retinanet, optimizer, scheduler, hyper_params[\"checkpoint_dir\"])\n",
    "\n",
    "retinanet, optimizer = amp.initialize(retinanet, optimizer, opt_level=\"O1\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using Multiple GPUs\")\n",
    "    retinanet = torch.nn.DataParallel(retinanet, device_ids=range(torch.cuda.device_count()))\n",
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "\n",
    "cb = Callback(project_name, experiment_name, hyper_params, hyper_params[\"save_dir\"])\n",
    "\n",
    "eval_params = {\n",
    "    \"overlap\":hyper_params[\"overlap\"],\n",
    "    \"cls_thresh\":hyper_params[\"cls_thresh\"]\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "        retinanet, \n",
    "        train_dl, \n",
    "        valid_dl, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        criterion, \n",
    "        hyper_params[\"device\"],\n",
    "        inferencer, \n",
    "        hyper_params[\"num_classes\"],\n",
    "        eval_params,\n",
    "        cb,\n",
    "        vis\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Started Epoch: 0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "0:00:22.796237 : 0 / 100\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "0:00:37.043305 : 10 / 100\n",
      "0:00:51.474983 : 20 / 100\n",
      "0:01:05.384628 : 30 / 100\n",
      "0:01:19.276098 : 40 / 100\n",
      "0:01:33.274959 : 50 / 100\n",
      "0:01:47.184525 : 60 / 100\n",
      "0:02:01.041681 : 70 / 100\n",
      "0:02:15.022724 : 80 / 100\n",
      "0:02:28.926209 : 90 / 100\n",
      "Ended Train Epoch in (hours): 0:02:41.489188\n",
      "[VALID] 0:00:02.864453 : 0 / 100\n",
      "[VALID] 0:00:11.996232 : 10 / 100\n",
      "[VALID] 0:00:20.886543 : 20 / 100\n",
      "[VALID] 0:00:29.727993 : 30 / 100\n",
      "[VALID] 0:00:38.565853 : 40 / 100\n",
      "[VALID] 0:00:47.378727 : 50 / 100\n",
      "[VALID] 0:00:56.335509 : 60 / 100\n",
      "[VALID] 0:01:05.119619 : 70 / 100\n",
      "[VALID] 0:01:13.862506 : 80 / 100\n",
      "[VALID] 0:01:22.567850 : 90 / 100\n",
      "Epoch completed in (hours): 0:04:12.824384\n",
      "Training completed in (hours): 0:04:16.466758\n"
     ]
    }
   ],
   "source": [
    "trainer.train(hyper_params[\"epochs\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
