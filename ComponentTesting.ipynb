{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from dataloader import get_dataloader\n",
    "from preprocess import transformer, img_transform, reverse_img_transform\n",
    "from models import RetinaNet\n",
    "from loss import FocalLoss\n",
    "from train import Trainer, load_components\n",
    "from inference import InferenceTransform\n",
    "from evaluation import support_evaluate_model\n",
    "from utils import Visualiser\n",
    "from callbacks import Callback\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"gatletag/Local-Object-Detection-Tests\"\n",
    "experiment_name = \"local_test1\"\n",
    "\n",
    "dir_params = {\n",
    "    \"train_images_dir\": \"dataset/validation\",\n",
    "    \"train_bbox_dir\": \"data_info/valid/annotations/valid-anno.json\",\n",
    "    \"train_idx_to_id_dir\": \"data_info/valid/annotations/valid-idx_to_id.pkl\",\n",
    "\n",
    "    \"valid_images_dir\": \"dataset/validation\",\n",
    "    \"valid_bbox_dir\": \"data_info/valid/annotations/valid-anno.json\",\n",
    "    \"valid_idx_to_id_dir\": \"data_info/valid/annotations/valid-idx_to_id.pkl\",\n",
    "\n",
    "    \"clsids_to_idx_dir\": \"data_info/clsids_to_idx.pkl\",\n",
    "    \"clsids_to_names_dir\": \"data_info/clsids_to_names.pkl\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    # speed parameters\n",
    "    \"num_workers\": 8,\n",
    "    \"device\": \"cuda\",\n",
    "\n",
    "    # dataloader parameters\n",
    "    \"bs\": 16,\n",
    "    \"img_dim\": 512,\n",
    "\n",
    "    # anchor parameters\n",
    "    \"ratios\": [1/3, 1/2, 1, 2],\n",
    "    \"scales\": [0.25, 1, 2],\n",
    "\n",
    "    # network parameters\n",
    "    \"backbone\": \"resnet50\",\n",
    "    \"num_classes\": 501,\n",
    "    \"pretrained\": True,\n",
    "    \"freeze_bn\": True,\n",
    "\n",
    "    # loss parameters\n",
    "    \"alpha\": 0.25,\n",
    "    \"gamma\": 2.0,\n",
    "    \"IoU_bkgrd\":0.4,\n",
    "    \"IoU_pos\":0.5,\n",
    "    \"regress_factor\": [0.1, 0.1, 0.2, 0.2],\n",
    "\n",
    "    # optimizer parameters\n",
    "    \"lr\": 0.0003,\n",
    "    \"min_lr\": 0.000001,\n",
    "    \"patience\": 100,\n",
    "    \"decay_factor\": 0.3,\n",
    "\n",
    "    # training parameters\n",
    "    \"epochs\": 1,\n",
    "    \"checkpoint_dir\": \"temp3/final.pth\",\n",
    "    \"save_dir\": \"temp4\",\n",
    "    \"fine_tune\": False,\n",
    "\n",
    "    # evaluation parameters\n",
    "    \"cls_thresh\":0.10, \n",
    "    \"overlap\":0.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Class Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsids_to_names = pickle.load(open(dir_params[\"clsids_to_names_dir\"],'rb'))\n",
    "clsids_to_idx = pickle.load(open(dir_params[\"clsids_to_idx_dir\"],'rb'))\n",
    "idx_to_cls_ids = {v: k for k, v in clsids_to_idx.items()}\n",
    "idx_to_names = {k: clsids_to_names[v] for k, v in idx_to_cls_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Dataset Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": int(hyper_params[\"img_dim\"]*1.05), \"width\": int(hyper_params[\"img_dim\"]*1.05)}),\n",
    "        iaa.GammaContrast((0.9,1.1)),\n",
    "        iaa.Affine(rotate=(-5, 5), scale=(0.90, 1.10)),\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.00)),\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "train_transform_fn = transformer(train_seq, img_transform)\n",
    "\n",
    "train_dl = get_dataloader(\n",
    "        dir_params[\"train_images_dir\"], dir_params[\"train_bbox_dir\"], \n",
    "        dir_params[\"train_idx_to_id_dir\"], clsids_to_idx,\n",
    "        train_transform_fn, hyper_params[\"bs\"], True, hyper_params[\"num_workers\"], True\n",
    "    )\n",
    "\n",
    "valid_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "valid_transform_fn = transformer(valid_seq, img_transform)\n",
    "\n",
    "valid_dl = get_dataloader(\n",
    "        dir_params[\"valid_images_dir\"], \n",
    "        dir_params[\"valid_bbox_dir\"], \n",
    "        dir_params[\"valid_idx_to_id_dir\"], \n",
    "        clsids_to_idx, \n",
    "        valid_transform_fn, hyper_params[\"bs\"], False, hyper_params[\"num_workers\"], False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Loss Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(\n",
    "        hyper_params[\"alpha\"], \n",
    "        hyper_params[\"gamma\"], \n",
    "        hyper_params[\"IoU_bkgrd\"], \n",
    "        hyper_params[\"IoU_pos\"], \n",
    "        hyper_params[\"regress_factor\"], \n",
    "        hyper_params[\"device\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet = RetinaNet(\n",
    "        hyper_params[\"backbone\"], \n",
    "        hyper_params[\"num_classes\"],\n",
    "        hyper_params[\"ratios\"], \n",
    "        hyper_params[\"scales\"], \n",
    "        device=hyper_params[\"device\"], \n",
    "        pretrained = hyper_params[\"pretrained\"], \n",
    "        freeze_bn = hyper_params[\"freeze_bn\"],\n",
    "        prior=0.01, \n",
    "        feature_size=256, \n",
    "        pyramid_levels = [3, 4, 5, 6, 7],\n",
    "        criterion=criterion\n",
    "    )\n",
    "\n",
    "# TODO: Move this over to training file\n",
    "def set_parameter_requires_grad(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if (name.split('.')[0]) not in [\"fpn\", \"regressionModel\", \"classificationModel\"]:\n",
    "                param.requires_grad = False\n",
    "\n",
    "if hyper_params[\"fine_tune\"]==True:\n",
    "    set_parameter_requires_grad(retinanet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Optimizer Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(retinanet.parameters(), lr=hyper_params[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "        mode='min', \n",
    "        factor=hyper_params['decay_factor'], \n",
    "        patience=hyper_params[\"patience\"], \n",
    "        verbose=True, \n",
    "        min_lr=hyper_params[\"min_lr\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Visualisation Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualiser(\n",
    "        hyper_params[\"num_classes\"],\n",
    "        idx_to_names,\n",
    "        reverse_img_transform\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Inference Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferenceTransform(\n",
    "        idx_to_names,\n",
    "        idx_to_cls_ids,\n",
    "        hyper_params[\"regress_factor\"]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: temp3/final.pth\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Using Multiple GPUs\n",
      "LOD-83\n",
      "https://ui.neptune.ml/gatletag/Local-Object-Detection-Tests/e/LOD-83\n"
     ]
    }
   ],
   "source": [
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "load_components(retinanet, optimizer, scheduler, hyper_params[\"checkpoint_dir\"])\n",
    "\n",
    "retinanet, optimizer = amp.initialize(retinanet, optimizer, opt_level=\"O1\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using Multiple GPUs\")\n",
    "    retinanet = torch.nn.DataParallel(retinanet, device_ids=range(torch.cuda.device_count()))\n",
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "\n",
    "cb = Callback(project_name, experiment_name, hyper_params, hyper_params[\"save_dir\"])\n",
    "\n",
    "eval_params = {\n",
    "    \"overlap\":hyper_params[\"overlap\"],\n",
    "    \"cls_thresh\":hyper_params[\"cls_thresh\"]\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "        retinanet, \n",
    "        train_dl, \n",
    "        valid_dl, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        criterion, \n",
    "        hyper_params[\"device\"],\n",
    "        inferencer, \n",
    "        hyper_params[\"num_classes\"],\n",
    "        eval_params,\n",
    "        cb,\n",
    "        vis\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Started Epoch: 0\n",
      "0:00:20.439681 : 0 / 2182\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "0:00:26.965808 : 10 / 2182\n",
      "0:00:32.636722 : 20 / 2182\n",
      "0:00:38.392405 : 30 / 2182\n",
      "0:00:44.283270 : 40 / 2182\n",
      "0:00:50.039885 : 50 / 2182\n",
      "0:00:55.830644 : 60 / 2182\n",
      "0:01:01.469948 : 70 / 2182\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "0:01:07.296155 : 80 / 2182\n",
      "0:01:13.135126 : 90 / 2182\n",
      "0:01:18.895243 : 100 / 2182\n",
      "0:01:24.731936 : 110 / 2182\n",
      "0:01:30.416616 : 120 / 2182\n",
      "0:01:36.172319 : 130 / 2182\n",
      "0:01:41.983930 : 140 / 2182\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "0:01:47.774008 : 150 / 2182\n",
      "0:01:53.527762 : 160 / 2182\n",
      "0:01:59.264256 : 170 / 2182\n",
      "0:02:05.148280 : 180 / 2182\n",
      "0:02:10.896548 : 190 / 2182\n",
      "0:02:16.518932 : 200 / 2182\n",
      "0:02:22.245612 : 210 / 2182\n",
      "0:02:27.905260 : 220 / 2182\n",
      "0:02:33.556727 : 230 / 2182\n",
      "0:02:39.219760 : 240 / 2182\n",
      "0:02:45.100806 : 250 / 2182\n",
      "0:02:50.765645 : 260 / 2182\n",
      "0:02:56.450635 : 270 / 2182\n",
      "0:03:02.081659 : 280 / 2182\n",
      "0:03:07.931808 : 290 / 2182\n",
      "0:03:13.662611 : 300 / 2182\n",
      "0:03:19.317407 : 310 / 2182\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "0:03:24.958299 : 320 / 2182\n",
      "0:03:30.690086 : 330 / 2182\n",
      "0:03:36.530621 : 340 / 2182\n",
      "0:03:42.421746 : 350 / 2182\n",
      "0:03:48.218944 : 360 / 2182\n",
      "0:03:53.897445 : 370 / 2182\n",
      "0:03:59.525707 : 380 / 2182\n",
      "0:04:05.249574 : 390 / 2182\n",
      "0:04:10.961683 : 400 / 2182\n",
      "0:04:16.809018 : 410 / 2182\n",
      "0:04:22.464740 : 420 / 2182\n",
      "0:04:28.274755 : 430 / 2182\n",
      "0:04:34.066296 : 440 / 2182\n",
      "0:04:39.833051 : 450 / 2182\n",
      "0:04:45.552297 : 460 / 2182\n",
      "0:04:51.194474 : 470 / 2182\n",
      "0:04:56.963032 : 480 / 2182\n",
      "0:05:02.894817 : 490 / 2182\n",
      "0:05:08.707181 : 500 / 2182\n",
      "0:05:14.417430 : 510 / 2182\n",
      "0:05:20.136474 : 520 / 2182\n",
      "0:05:25.940459 : 530 / 2182\n",
      "0:05:31.745218 : 540 / 2182\n",
      "0:05:37.551364 : 550 / 2182\n",
      "0:05:43.427188 : 560 / 2182\n",
      "0:05:49.275910 : 570 / 2182\n",
      "0:05:54.910219 : 580 / 2182\n",
      "0:06:00.598128 : 590 / 2182\n",
      "0:06:06.365798 : 600 / 2182\n",
      "0:06:12.154609 : 610 / 2182\n",
      "0:06:17.963926 : 620 / 2182\n",
      "0:06:23.640499 : 630 / 2182\n",
      "0:06:29.303509 : 640 / 2182\n",
      "0:06:35.094770 : 650 / 2182\n",
      "0:06:40.860564 : 660 / 2182\n",
      "0:06:46.568861 : 670 / 2182\n",
      "0:06:52.284338 : 680 / 2182\n",
      "0:06:57.919067 : 690 / 2182\n",
      "0:07:03.698582 : 700 / 2182\n",
      "0:07:09.548728 : 710 / 2182\n",
      "0:07:15.513954 : 720 / 2182\n",
      "0:07:21.251451 : 730 / 2182\n",
      "0:07:27.054678 : 740 / 2182\n",
      "0:07:32.828159 : 750 / 2182\n",
      "0:07:38.606099 : 760 / 2182\n",
      "0:07:44.468507 : 770 / 2182\n",
      "0:07:50.291835 : 780 / 2182\n",
      "0:07:56.040227 : 790 / 2182\n",
      "0:08:01.860288 : 800 / 2182\n",
      "0:08:07.585699 : 810 / 2182\n",
      "0:08:13.250704 : 820 / 2182\n",
      "0:08:19.037313 : 830 / 2182\n",
      "0:08:24.745176 : 840 / 2182\n",
      "0:08:30.517037 : 850 / 2182\n",
      "0:08:36.301191 : 860 / 2182\n",
      "0:08:42.021159 : 870 / 2182\n",
      "0:08:47.706742 : 880 / 2182\n",
      "0:08:53.481096 : 890 / 2182\n",
      "0:08:59.231348 : 900 / 2182\n",
      "0:09:05.147108 : 910 / 2182\n",
      "0:09:11.002027 : 920 / 2182\n",
      "0:09:16.781064 : 930 / 2182\n",
      "0:09:22.534571 : 940 / 2182\n",
      "0:09:28.249802 : 950 / 2182\n",
      "0:09:33.907947 : 960 / 2182\n",
      "0:09:39.737190 : 970 / 2182\n",
      "0:09:45.452973 : 980 / 2182\n",
      "0:09:51.184251 : 990 / 2182\n",
      "0:09:57.104275 : 1000 / 2182\n",
      "0:10:02.891939 : 1010 / 2182\n",
      "0:10:08.585842 : 1020 / 2182\n",
      "0:10:14.318925 : 1030 / 2182\n",
      "0:10:20.054961 : 1040 / 2182\n",
      "0:10:25.874938 : 1050 / 2182\n",
      "0:10:31.652624 : 1060 / 2182\n",
      "0:10:37.385436 : 1070 / 2182\n",
      "0:10:43.141290 : 1080 / 2182\n",
      "0:10:48.935490 : 1090 / 2182\n",
      "0:10:54.859845 : 1100 / 2182\n",
      "0:11:00.611524 : 1110 / 2182\n",
      "0:11:06.395521 : 1120 / 2182\n",
      "0:11:12.057337 : 1130 / 2182\n",
      "0:11:17.795021 : 1140 / 2182\n",
      "0:11:23.620205 : 1150 / 2182\n",
      "0:11:29.518357 : 1160 / 2182\n",
      "0:11:35.239528 : 1170 / 2182\n",
      "0:11:40.763984 : 1180 / 2182\n",
      "0:11:46.588279 : 1190 / 2182\n",
      "0:11:52.301044 : 1200 / 2182\n",
      "0:11:58.000635 : 1210 / 2182\n",
      "0:12:03.737059 : 1220 / 2182\n",
      "0:12:09.495818 : 1230 / 2182\n",
      "0:12:15.164747 : 1240 / 2182\n",
      "0:12:20.886895 : 1250 / 2182\n",
      "0:12:26.628647 : 1260 / 2182\n",
      "0:12:32.402522 : 1270 / 2182\n",
      "0:12:38.063990 : 1280 / 2182\n",
      "0:12:43.753237 : 1290 / 2182\n",
      "0:12:49.532095 : 1300 / 2182\n",
      "0:12:55.145175 : 1310 / 2182\n",
      "0:13:00.926005 : 1320 / 2182\n",
      "0:13:06.853998 : 1330 / 2182\n",
      "0:13:12.612770 : 1340 / 2182\n",
      "0:13:18.419294 : 1350 / 2182\n",
      "0:13:24.209362 : 1360 / 2182\n",
      "0:13:29.865159 : 1370 / 2182\n",
      "0:13:35.690640 : 1380 / 2182\n",
      "0:13:41.471389 : 1390 / 2182\n",
      "0:13:47.324347 : 1400 / 2182\n",
      "0:13:53.083898 : 1410 / 2182\n",
      "0:13:58.771349 : 1420 / 2182\n",
      "0:14:04.571684 : 1430 / 2182\n",
      "0:14:10.466914 : 1440 / 2182\n",
      "0:14:16.249344 : 1450 / 2182\n",
      "0:14:21.897939 : 1460 / 2182\n",
      "0:14:27.595335 : 1470 / 2182\n",
      "0:14:33.462267 : 1480 / 2182\n",
      "0:14:39.093146 : 1490 / 2182\n",
      "0:14:44.968844 : 1500 / 2182\n",
      "0:14:50.843201 : 1510 / 2182\n",
      "0:14:56.769245 : 1520 / 2182\n",
      "0:15:02.332975 : 1530 / 2182\n",
      "0:15:07.948689 : 1540 / 2182\n",
      "0:15:13.702213 : 1550 / 2182\n",
      "0:15:19.514121 : 1560 / 2182\n",
      "0:15:25.252767 : 1570 / 2182\n",
      "0:15:30.939244 : 1580 / 2182\n",
      "0:15:36.832768 : 1590 / 2182\n",
      "0:15:42.584406 : 1600 / 2182\n",
      "0:15:48.159245 : 1610 / 2182\n",
      "0:15:53.908394 : 1620 / 2182\n",
      "0:15:59.481489 : 1630 / 2182\n",
      "0:16:05.298912 : 1640 / 2182\n",
      "0:16:11.084503 : 1650 / 2182\n",
      "0:16:16.808142 : 1660 / 2182\n",
      "0:16:22.580912 : 1670 / 2182\n",
      "0:16:28.456156 : 1680 / 2182\n",
      "0:16:34.197831 : 1690 / 2182\n",
      "0:16:39.936486 : 1700 / 2182\n",
      "0:16:45.733051 : 1710 / 2182\n",
      "0:16:51.531368 : 1720 / 2182\n",
      "0:16:57.174036 : 1730 / 2182\n",
      "0:17:02.874325 : 1740 / 2182\n",
      "0:17:08.569268 : 1750 / 2182\n",
      "0:17:14.313391 : 1760 / 2182\n",
      "0:17:20.142122 : 1770 / 2182\n",
      "0:17:25.828926 : 1780 / 2182\n",
      "0:17:31.522093 : 1790 / 2182\n",
      "0:17:37.229777 : 1800 / 2182\n",
      "0:17:42.916438 : 1810 / 2182\n",
      "0:17:48.640103 : 1820 / 2182\n",
      "0:17:54.386042 : 1830 / 2182\n",
      "0:18:00.090924 : 1840 / 2182\n",
      "0:18:05.763809 : 1850 / 2182\n",
      "0:18:11.707767 : 1860 / 2182\n",
      "0:18:17.394756 : 1870 / 2182\n",
      "0:18:23.067273 : 1880 / 2182\n",
      "0:18:28.883796 : 1890 / 2182\n",
      "0:18:34.564729 : 1900 / 2182\n",
      "0:18:40.284218 : 1910 / 2182\n",
      "0:18:46.086668 : 1920 / 2182\n",
      "0:18:51.797911 : 1930 / 2182\n",
      "0:18:57.442244 : 1940 / 2182\n",
      "0:19:03.346304 : 1950 / 2182\n",
      "0:19:09.065850 : 1960 / 2182\n",
      "0:19:14.871302 : 1970 / 2182\n",
      "0:19:20.638251 : 1980 / 2182\n",
      "0:19:26.305362 : 1990 / 2182\n",
      "0:19:32.113340 : 2000 / 2182\n",
      "0:19:37.855370 : 2010 / 2182\n",
      "0:19:43.700900 : 2020 / 2182\n",
      "0:19:49.531696 : 2030 / 2182\n",
      "0:19:55.155174 : 2040 / 2182\n",
      "0:20:01.012705 : 2050 / 2182\n",
      "0:20:06.712896 : 2060 / 2182\n",
      "0:20:12.453141 : 2070 / 2182\n",
      "0:20:18.171934 : 2080 / 2182\n",
      "0:20:23.956822 : 2090 / 2182\n",
      "0:20:29.624128 : 2100 / 2182\n",
      "0:20:35.435715 : 2110 / 2182\n",
      "0:20:41.238660 : 2120 / 2182\n",
      "0:20:47.007403 : 2130 / 2182\n",
      "0:20:52.711367 : 2140 / 2182\n",
      "0:20:58.520540 : 2150 / 2182\n",
      "0:21:04.212829 : 2160 / 2182\n",
      "0:21:09.627474 : 2170 / 2182\n",
      "0:21:14.632161 : 2180 / 2182\n",
      "Ended Train Epoch in (hours): 0:21:15.233627\n",
      "[VALID] 0:00:02.309678 : 0 / 2183\n",
      "[VALID] 0:00:06.892177 : 10 / 2183\n",
      "[VALID] 0:00:11.199752 : 20 / 2183\n",
      "[VALID] 0:00:15.676069 : 30 / 2183\n",
      "[VALID] 0:00:19.789181 : 40 / 2183\n",
      "[VALID] 0:00:23.953817 : 50 / 2183\n",
      "[VALID] 0:00:28.206631 : 60 / 2183\n",
      "[VALID] 0:00:32.526053 : 70 / 2183\n",
      "[VALID] 0:00:36.749756 : 80 / 2183\n",
      "[VALID] 0:00:40.995516 : 90 / 2183\n",
      "[VALID] 0:00:45.253242 : 100 / 2183\n",
      "[VALID] 0:00:49.761612 : 110 / 2183\n",
      "[VALID] 0:00:54.098908 : 120 / 2183\n",
      "[VALID] 0:00:58.440051 : 130 / 2183\n",
      "[VALID] 0:01:02.698839 : 140 / 2183\n",
      "[VALID] 0:01:06.892361 : 150 / 2183\n",
      "[VALID] 0:01:10.976792 : 160 / 2183\n",
      "[VALID] 0:01:15.133198 : 170 / 2183\n",
      "[VALID] 0:01:19.683872 : 180 / 2183\n",
      "[VALID] 0:01:23.889965 : 190 / 2183\n",
      "[VALID] 0:01:28.146410 : 200 / 2183\n",
      "[VALID] 0:01:32.292200 : 210 / 2183\n",
      "[VALID] 0:01:36.441189 : 220 / 2183\n",
      "[VALID] 0:01:40.821562 : 230 / 2183\n",
      "[VALID] 0:01:45.102473 : 240 / 2183\n",
      "[VALID] 0:01:49.229193 : 250 / 2183\n",
      "[VALID] 0:01:53.793029 : 260 / 2183\n",
      "[VALID] 0:01:58.004612 : 270 / 2183\n",
      "[VALID] 0:02:02.237890 : 280 / 2183\n",
      "[VALID] 0:02:06.431812 : 290 / 2183\n",
      "[VALID] 0:02:10.623153 : 300 / 2183\n",
      "[VALID] 0:02:15.087151 : 310 / 2183\n",
      "[VALID] 0:02:19.227304 : 320 / 2183\n",
      "[VALID] 0:02:23.443760 : 330 / 2183\n",
      "[VALID] 0:02:27.787597 : 340 / 2183\n",
      "[VALID] 0:02:32.255485 : 350 / 2183\n",
      "[VALID] 0:02:36.383944 : 360 / 2183\n",
      "[VALID] 0:02:40.562606 : 370 / 2183\n",
      "[VALID] 0:02:44.867849 : 380 / 2183\n",
      "[VALID] 0:02:49.113954 : 390 / 2183\n",
      "[VALID] 0:02:53.353766 : 400 / 2183\n",
      "[VALID] 0:02:57.646906 : 410 / 2183\n",
      "[VALID] 0:03:01.687289 : 420 / 2183\n",
      "[VALID] 0:03:06.055825 : 430 / 2183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VALID] 0:03:10.460506 : 440 / 2183\n",
      "[VALID] 0:03:14.777974 : 450 / 2183\n",
      "[VALID] 0:03:19.081153 : 460 / 2183\n",
      "[VALID] 0:03:23.221529 : 470 / 2183\n",
      "[VALID] 0:03:27.423603 : 480 / 2183\n",
      "[VALID] 0:03:31.840870 : 490 / 2183\n",
      "[VALID] 0:03:35.990251 : 500 / 2183\n",
      "[VALID] 0:03:40.147353 : 510 / 2183\n",
      "[VALID] 0:03:44.428569 : 520 / 2183\n",
      "[VALID] 0:03:48.887496 : 530 / 2183\n",
      "[VALID] 0:03:53.155668 : 540 / 2183\n",
      "[VALID] 0:03:57.391175 : 550 / 2183\n",
      "[VALID] 0:04:01.775405 : 560 / 2183\n",
      "[VALID] 0:04:05.922627 : 570 / 2183\n",
      "[VALID] 0:04:10.209390 : 580 / 2183\n",
      "[VALID] 0:04:14.282432 : 590 / 2183\n",
      "[VALID] 0:04:18.627472 : 600 / 2183\n",
      "[VALID] 0:04:22.759677 : 610 / 2183\n",
      "[VALID] 0:04:26.879634 : 620 / 2183\n",
      "[VALID] 0:04:31.198412 : 630 / 2183\n",
      "[VALID] 0:04:35.314563 : 640 / 2183\n",
      "[VALID] 0:04:39.554838 : 650 / 2183\n",
      "[VALID] 0:04:43.615963 : 660 / 2183\n",
      "[VALID] 0:04:47.912707 : 670 / 2183\n",
      "[VALID] 0:04:52.215635 : 680 / 2183\n",
      "[VALID] 0:04:56.518359 : 690 / 2183\n",
      "[VALID] 0:05:00.806536 : 700 / 2183\n",
      "[VALID] 0:05:04.978863 : 710 / 2183\n",
      "[VALID] 0:05:09.464518 : 720 / 2183\n",
      "[VALID] 0:05:13.699532 : 730 / 2183\n",
      "[VALID] 0:05:18.053842 : 740 / 2183\n",
      "[VALID] 0:05:22.267836 : 750 / 2183\n",
      "[VALID] 0:05:26.496109 : 760 / 2183\n",
      "[VALID] 0:05:30.994200 : 770 / 2183\n",
      "[VALID] 0:05:35.367768 : 780 / 2183\n",
      "[VALID] 0:05:39.510566 : 790 / 2183\n",
      "[VALID] 0:05:43.904715 : 800 / 2183\n",
      "[VALID] 0:05:47.992886 : 810 / 2183\n",
      "[VALID] 0:05:52.304017 : 820 / 2183\n",
      "[VALID] 0:05:56.490570 : 830 / 2183\n",
      "[VALID] 0:06:00.826318 : 840 / 2183\n",
      "[VALID] 0:06:05.264354 : 850 / 2183\n",
      "[VALID] 0:06:09.487135 : 860 / 2183\n",
      "[VALID] 0:06:13.741132 : 870 / 2183\n",
      "[VALID] 0:06:18.064069 : 880 / 2183\n",
      "[VALID] 0:06:22.238774 : 890 / 2183\n",
      "[VALID] 0:06:26.481030 : 900 / 2183\n",
      "[VALID] 0:06:30.666601 : 910 / 2183\n",
      "[VALID] 0:06:35.006042 : 920 / 2183\n",
      "[VALID] 0:06:39.121888 : 930 / 2183\n",
      "[VALID] 0:06:43.353939 : 940 / 2183\n",
      "[VALID] 0:06:47.697208 : 950 / 2183\n",
      "[VALID] 0:06:51.994153 : 960 / 2183\n",
      "[VALID] 0:06:56.305302 : 970 / 2183\n",
      "[VALID] 0:07:00.473828 : 980 / 2183\n",
      "[VALID] 0:07:04.840382 : 990 / 2183\n",
      "[VALID] 0:07:09.060134 : 1000 / 2183\n",
      "[VALID] 0:07:13.120501 : 1010 / 2183\n",
      "[VALID] 0:07:17.559513 : 1020 / 2183\n",
      "[VALID] 0:07:21.842321 : 1030 / 2183\n",
      "[VALID] 0:07:26.098797 : 1040 / 2183\n",
      "[VALID] 0:07:30.239827 : 1050 / 2183\n",
      "[VALID] 0:07:34.634818 : 1060 / 2183\n",
      "[VALID] 0:07:38.883812 : 1070 / 2183\n",
      "[VALID] 0:07:43.028648 : 1080 / 2183\n",
      "[VALID] 0:07:47.302366 : 1090 / 2183\n",
      "[VALID] 0:07:51.433715 : 1100 / 2183\n",
      "[VALID] 0:07:55.684179 : 1110 / 2183\n",
      "[VALID] 0:08:00.152060 : 1120 / 2183\n",
      "[VALID] 0:08:04.334730 : 1130 / 2183\n",
      "[VALID] 0:08:08.620812 : 1140 / 2183\n",
      "[VALID] 0:08:12.709571 : 1150 / 2183\n",
      "[VALID] 0:08:16.927412 : 1160 / 2183\n",
      "[VALID] 0:08:21.036930 : 1170 / 2183\n",
      "[VALID] 0:08:25.217535 : 1180 / 2183\n",
      "[VALID] 0:08:29.617863 : 1190 / 2183\n",
      "[VALID] 0:08:33.803678 : 1200 / 2183\n",
      "[VALID] 0:08:38.088227 : 1210 / 2183\n",
      "[VALID] 0:08:42.350121 : 1220 / 2183\n",
      "[VALID] 0:08:46.603711 : 1230 / 2183\n",
      "[VALID] 0:08:51.020206 : 1240 / 2183\n",
      "[VALID] 0:08:55.337886 : 1250 / 2183\n",
      "[VALID] 0:08:59.693428 : 1260 / 2183\n",
      "[VALID] 0:09:03.993376 : 1270 / 2183\n",
      "[VALID] 0:09:08.322570 : 1280 / 2183\n",
      "[VALID] 0:09:12.826587 : 1290 / 2183\n",
      "[VALID] 0:09:16.994198 : 1300 / 2183\n",
      "[VALID] 0:09:21.399047 : 1310 / 2183\n",
      "[VALID] 0:09:25.623206 : 1320 / 2183\n",
      "[VALID] 0:09:29.963094 : 1330 / 2183\n",
      "[VALID] 0:09:34.280600 : 1340 / 2183\n",
      "[VALID] 0:09:38.423106 : 1350 / 2183\n",
      "[VALID] 0:09:42.543247 : 1360 / 2183\n",
      "[VALID] 0:09:46.785542 : 1370 / 2183\n",
      "[VALID] 0:09:51.282057 : 1380 / 2183\n",
      "[VALID] 0:09:55.525917 : 1390 / 2183\n",
      "[VALID] 0:09:59.779783 : 1400 / 2183\n",
      "[VALID] 0:10:04.168296 : 1410 / 2183\n",
      "[VALID] 0:10:08.326360 : 1420 / 2183\n",
      "[VALID] 0:10:12.644737 : 1430 / 2183\n",
      "[VALID] 0:10:17.018171 : 1440 / 2183\n",
      "[VALID] 0:10:21.331111 : 1450 / 2183\n",
      "[VALID] 0:10:25.646487 : 1460 / 2183\n",
      "[VALID] 0:10:30.099566 : 1470 / 2183\n",
      "[VALID] 0:10:34.384388 : 1480 / 2183\n",
      "[VALID] 0:10:38.739719 : 1490 / 2183\n",
      "[VALID] 0:10:43.069721 : 1500 / 2183\n",
      "[VALID] 0:10:47.307362 : 1510 / 2183\n",
      "[VALID] 0:10:51.468252 : 1520 / 2183\n",
      "[VALID] 0:10:55.910211 : 1530 / 2183\n",
      "[VALID] 0:11:00.141831 : 1540 / 2183\n",
      "[VALID] 0:11:04.196313 : 1550 / 2183\n",
      "[VALID] 0:11:08.495144 : 1560 / 2183\n",
      "[VALID] 0:11:12.827210 : 1570 / 2183\n",
      "[VALID] 0:11:17.167772 : 1580 / 2183\n",
      "[VALID] 0:11:21.586789 : 1590 / 2183\n",
      "[VALID] 0:11:26.012124 : 1600 / 2183\n",
      "[VALID] 0:11:30.195465 : 1610 / 2183\n",
      "[VALID] 0:11:34.424390 : 1620 / 2183\n",
      "[VALID] 0:11:38.730879 : 1630 / 2183\n",
      "[VALID] 0:11:43.068014 : 1640 / 2183\n",
      "[VALID] 0:11:47.458014 : 1650 / 2183\n",
      "[VALID] 0:11:51.850115 : 1660 / 2183\n",
      "[VALID] 0:11:56.148602 : 1670 / 2183\n",
      "[VALID] 0:12:00.358321 : 1680 / 2183\n",
      "[VALID] 0:12:04.603180 : 1690 / 2183\n",
      "[VALID] 0:12:08.827673 : 1700 / 2183\n",
      "[VALID] 0:12:14.006799 : 1710 / 2183\n",
      "[VALID] 0:12:18.478894 : 1720 / 2183\n",
      "[VALID] 0:12:22.593370 : 1730 / 2183\n",
      "[VALID] 0:12:26.992129 : 1740 / 2183\n",
      "[VALID] 0:12:31.384992 : 1750 / 2183\n",
      "[VALID] 0:12:35.693136 : 1760 / 2183\n",
      "[VALID] 0:12:39.846833 : 1770 / 2183\n",
      "[VALID] 0:12:43.973435 : 1780 / 2183\n",
      "[VALID] 0:12:48.233771 : 1790 / 2183\n",
      "[VALID] 0:12:52.510646 : 1800 / 2183\n",
      "[VALID] 0:12:56.970356 : 1810 / 2183\n",
      "[VALID] 0:13:01.299982 : 1820 / 2183\n",
      "[VALID] 0:13:05.500810 : 1830 / 2183\n",
      "[VALID] 0:13:09.836737 : 1840 / 2183\n",
      "[VALID] 0:13:14.155821 : 1850 / 2183\n",
      "[VALID] 0:13:18.559497 : 1860 / 2183\n",
      "[VALID] 0:13:22.929121 : 1870 / 2183\n",
      "[VALID] 0:13:27.041039 : 1880 / 2183\n",
      "[VALID] 0:13:31.167778 : 1890 / 2183\n",
      "[VALID] 0:13:35.683125 : 1900 / 2183\n",
      "[VALID] 0:13:39.857104 : 1910 / 2183\n",
      "[VALID] 0:13:44.039530 : 1920 / 2183\n",
      "[VALID] 0:13:48.234119 : 1930 / 2183\n",
      "[VALID] 0:13:52.554007 : 1940 / 2183\n",
      "[VALID] 0:13:56.810381 : 1950 / 2183\n",
      "[VALID] 0:14:01.115175 : 1960 / 2183\n",
      "[VALID] 0:14:05.423923 : 1970 / 2183\n",
      "[VALID] 0:14:09.610059 : 1980 / 2183\n",
      "[VALID] 0:14:14.121683 : 1990 / 2183\n",
      "[VALID] 0:14:18.450336 : 2000 / 2183\n",
      "[VALID] 0:14:22.525076 : 2010 / 2183\n",
      "[VALID] 0:14:26.923787 : 2020 / 2183\n",
      "[VALID] 0:14:31.244168 : 2030 / 2183\n",
      "[VALID] 0:14:35.492824 : 2040 / 2183\n",
      "[VALID] 0:14:40.026198 : 2050 / 2183\n",
      "[VALID] 0:14:44.254958 : 2060 / 2183\n",
      "[VALID] 0:14:48.693637 : 2070 / 2183\n",
      "[VALID] 0:14:53.115274 : 2080 / 2183\n",
      "[VALID] 0:14:57.169258 : 2090 / 2183\n",
      "[VALID] 0:15:01.494067 : 2100 / 2183\n",
      "[VALID] 0:15:05.650138 : 2110 / 2183\n",
      "[VALID] 0:15:09.981354 : 2120 / 2183\n",
      "[VALID] 0:15:14.159660 : 2130 / 2183\n",
      "[VALID] 0:15:18.619209 : 2140 / 2183\n",
      "[VALID] 0:15:22.894199 : 2150 / 2183\n",
      "[VALID] 0:15:27.241952 : 2160 / 2183\n",
      "[VALID] 0:15:31.250552 : 2170 / 2183\n",
      "[VALID] 0:15:34.725092 : 2180 / 2183\n",
      "Epoch completed in (hours): 0:37:25.043453\n",
      "Training completed in (hours): 0:37:28.537630\n"
     ]
    }
   ],
   "source": [
    "trainer.train(hyper_params[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
