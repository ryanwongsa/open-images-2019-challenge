{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from dataloader import get_dataloader, get_balanced_dataloader\n",
    "from preprocess import transformer, img_transform, reverse_img_transform\n",
    "from models import RetinaNet\n",
    "from loss import FocalLoss\n",
    "from train import Trainer, load_components\n",
    "from inference import InferenceTransform\n",
    "from utils import Visualiser\n",
    "from callbacks import Callback\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import argparse\n",
    "\n",
    "gc.collect(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train Retinanet')\n",
    "\n",
    "# parser.add_argument('--config-file-dir', type=str, help='config file location')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "configs_dir = \"configs/balanced_subset0_config.json\" # args.config_file_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = json.load(open(configs_dir,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = parameters[\"hyperparams\"]\n",
    "dir_params = parameters[\"dirparams\"]\n",
    "project_params = parameters[\"projectconfig\"]\n",
    "\n",
    "project_name = project_params[\"project_name\"]\n",
    "experiment_name = project_params[\"experiment_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsids_to_names = json.load(open(dir_params[\"clsids_to_names_dir\"],'r'))\n",
    "clsids_to_idx = json.load(open(dir_params[\"clsids_to_idx_dir\"],'r'))\n",
    "idx_to_cls_ids = {v: k for k, v in clsids_to_idx.items()}\n",
    "idx_to_names = {k: clsids_to_names[v] for k, v in idx_to_cls_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": int(hyper_params[\"img_dim\"]*1.05), \"width\": int(hyper_params[\"img_dim\"]*1.05)}),\n",
    "        iaa.GammaContrast((0.9,1.1)),\n",
    "        iaa.Affine(rotate=(-5, 5), scale=(0.90, 1.10)),\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.00)),\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "train_transform_fn = transformer(train_seq, img_transform)\n",
    "train_dl = get_balanced_dataloader(\n",
    "        dir_params[\"train_images_dir\"], dir_params[\"train_bbox_dir\"], \n",
    "        dir_params[\"train_dict_clsid_to_list_imgs_dir\"], dir_params[\"dict_distributions\"], clsids_to_idx, dir_params[\"num_items\"],\n",
    "        train_transform_fn, hyper_params[\"bs\"], True, hyper_params[\"num_workers\"], True\n",
    "    )\n",
    "\n",
    "valid_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "valid_transform_fn = transformer(valid_seq, img_transform)\n",
    "valid_dl = get_dataloader(\n",
    "        dir_params[\"valid_images_dir\"], \n",
    "        dir_params[\"valid_bbox_dir\"], \n",
    "        dir_params[\"valid_idx_to_id_dir\"], \n",
    "        clsids_to_idx, \n",
    "        valid_transform_fn, hyper_params[\"bs\"], False, hyper_params[\"num_workers\"], False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(\n",
    "        hyper_params[\"alpha\"], \n",
    "        hyper_params[\"gamma\"], \n",
    "        hyper_params[\"IoU_bkgrd\"], \n",
    "        hyper_params[\"IoU_pos\"], \n",
    "        hyper_params[\"regress_factor\"], \n",
    "        hyper_params[\"device\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /home/ryancwonguk/.cache/torch/checkpoints/resnet101-5d3b4d8f.pth\n",
      "100%|██████████| 178728960/178728960 [00:04<00:00, 37769246.64it/s]\n"
     ]
    }
   ],
   "source": [
    "retinanet = RetinaNet(\n",
    "        hyper_params[\"backbone\"], \n",
    "        hyper_params[\"num_classes\"],\n",
    "        hyper_params[\"ratios\"], \n",
    "        hyper_params[\"scales\"], \n",
    "        device=hyper_params[\"device\"], \n",
    "        pretrained = hyper_params[\"pretrained\"], \n",
    "        freeze_bn = hyper_params[\"freeze_bn\"],\n",
    "        prior=0.01, \n",
    "        feature_size=256, \n",
    "        pyramid_levels = [3, 4, 5, 6, 7],\n",
    "        criterion=criterion\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if (name.split('.')[0]) not in [\"fpn\", \"regressionModel\", \"classificationModel\"]:\n",
    "                param.requires_grad = False\n",
    "\n",
    "if hyper_params[\"fine_tune\"]==True:\n",
    "    set_parameter_requires_grad(retinanet)\n",
    "retinanet = retinanet.to(hyper_params[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(retinanet.parameters(), lr=hyper_params[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "#         mode='min', \n",
    "#         factor=hyper_params['decay_factor'], \n",
    "#         patience=hyper_params[\"patience\"], \n",
    "#         verbose=True, \n",
    "#         min_lr=hyper_params[\"min_lr\"]\n",
    "#     )\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=hyper_params[\"lr\"], max_lr=hyper_params[\"lr\"]*10,step_size_up=int(len(train_dl)*hyper_params[\"epochs\"]*0.3), step_size_down=int(len(train_dl)*hyper_params[\"epochs\"]*0.7), mode='exp_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualiser(\n",
    "        hyper_params[\"num_classes\"],\n",
    "        idx_to_names,\n",
    "        reverse_img_transform\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferenceTransform(\n",
    "        idx_to_names,\n",
    "        idx_to_cls_ids,\n",
    "        hyper_params[\"regress_factor\"]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_components(retinanet, optimizer, scheduler, hyper_params[\"checkpoint_dir\"])\n",
    "retinanet, optimizer = amp.initialize(retinanet, optimizer, opt_level=\"O1\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using Multiple GPUs\")\n",
    "    retinanet = torch.nn.DataParallel(retinanet, device_ids=range(torch.cuda.device_count())) \n",
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "cb = Callback(project_name, experiment_name, hyper_params, hyper_params[\"save_dir\"])\n",
    "\n",
    "eval_params = {\n",
    "    \"overlap\":hyper_params[\"overlap\"],\n",
    "    \"cls_thresh\":hyper_params[\"cls_thresh\"]\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "        retinanet, \n",
    "        train_dl, \n",
    "        valid_dl, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        criterion, \n",
    "        hyper_params[\"device\"],\n",
    "        inferencer, \n",
    "        hyper_params[\"num_classes\"],\n",
    "        eval_params,\n",
    "        cb,\n",
    "        vis\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(hyper_params[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
