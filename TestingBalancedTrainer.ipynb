{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook/__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imgaug import augmenters as iaa\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "from dataloader import get_dataloader, get_balanced_dataloader\n",
    "from preprocess import transformer, img_transform, reverse_img_transform\n",
    "from models import RetinaNet\n",
    "from loss import FocalLoss\n",
    "from train import Trainer, load_components\n",
    "from inference import InferenceTransform\n",
    "from utils import Visualiser\n",
    "from callbacks import Callback\n",
    "import pickle\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import argparse\n",
    "\n",
    "gc.collect(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Train Retinanet')\n",
    "\n",
    "# parser.add_argument('--config-file-dir', type=str, help='config file location')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "configs_dir = \"configs/balanced_subset0_2_config.json\" # args.config_file_dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = json.load(open(configs_dir,'r'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = parameters[\"hyperparams\"]\n",
    "dir_params = parameters[\"dirparams\"]\n",
    "project_params = parameters[\"projectconfig\"]\n",
    "\n",
    "project_name = project_params[\"project_name\"]\n",
    "experiment_name = project_params[\"experiment_name\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clsids_to_names = json.load(open(dir_params[\"clsids_to_names_dir\"],'r'))\n",
    "clsids_to_idx = json.load(open(dir_params[\"clsids_to_idx_dir\"],'r'))\n",
    "idx_to_cls_ids = {v: k for k, v in clsids_to_idx.items()}\n",
    "idx_to_names = {k: clsids_to_names[v] for k, v in idx_to_cls_ids.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": int(hyper_params[\"img_dim\"]*1.05), \"width\": int(hyper_params[\"img_dim\"]*1.05)}),\n",
    "        iaa.GammaContrast((0.9,1.1)),\n",
    "        iaa.Affine(rotate=(-5, 5), scale=(0.90, 1.10)),\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.CropAndPad(percent=(-0.05, 0.00)),\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "train_transform_fn = transformer(train_seq, img_transform)\n",
    "train_dl = get_balanced_dataloader(\n",
    "        dir_params[\"train_images_dir\"], dir_params[\"train_bbox_dir\"], \n",
    "        dir_params[\"train_dict_clsid_to_list_imgs_dir\"], dir_params[\"dict_distributions\"], clsids_to_idx, dir_params[\"num_items\"],\n",
    "        train_transform_fn, hyper_params[\"bs\"], True, hyper_params[\"num_workers\"], True\n",
    "    )\n",
    "\n",
    "valid_seq = iaa.Sequential([\n",
    "        iaa.Resize({\"height\": hyper_params[\"img_dim\"], \"width\": hyper_params[\"img_dim\"]})\n",
    "    ])\n",
    "valid_transform_fn = transformer(valid_seq, img_transform)\n",
    "valid_dl = get_dataloader(\n",
    "        dir_params[\"valid_images_dir\"], \n",
    "        dir_params[\"valid_bbox_dir\"], \n",
    "        dir_params[\"valid_idx_to_id_dir\"], \n",
    "        clsids_to_idx, \n",
    "        valid_transform_fn, hyper_params[\"bs\"], False, hyper_params[\"num_workers\"], False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = FocalLoss(\n",
    "        hyper_params[\"alpha\"], \n",
    "        hyper_params[\"gamma\"], \n",
    "        hyper_params[\"IoU_bkgrd\"], \n",
    "        hyper_params[\"IoU_pos\"], \n",
    "        hyper_params[\"regress_factor\"], \n",
    "        hyper_params[\"device\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retinanet = RetinaNet(\n",
    "        hyper_params[\"backbone\"], \n",
    "        hyper_params[\"num_classes\"],\n",
    "        hyper_params[\"ratios\"], \n",
    "        hyper_params[\"scales\"], \n",
    "        device=hyper_params[\"device\"], \n",
    "        pretrained = hyper_params[\"pretrained\"], \n",
    "        freeze_bn = hyper_params[\"freeze_bn\"],\n",
    "        prior=0.01, \n",
    "        feature_size=256, \n",
    "        pyramid_levels = [3, 4, 5, 6, 7],\n",
    "        criterion=criterion\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model):\n",
    "        for name, param in model.named_parameters():\n",
    "            if (name.split('.')[0]) not in [\"fpn\", \"regressionModel\", \"classificationModel\"]:\n",
    "                param.requires_grad = False\n",
    "\n",
    "if hyper_params[\"fine_tune\"]==True:\n",
    "    set_parameter_requires_grad(retinanet)\n",
    "retinanet = retinanet.to(hyper_params[\"device\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(retinanet.parameters(), lr=hyper_params[\"lr\"], momentum=0.9, weight_decay=1e-4)\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "#         mode='min', \n",
    "#         factor=hyper_params['decay_factor'], \n",
    "#         patience=hyper_params[\"patience\"], \n",
    "#         verbose=True, \n",
    "#         min_lr=hyper_params[\"min_lr\"]\n",
    "#     )\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=hyper_params[\"lr\"], max_lr=hyper_params[\"lr\"]*10,step_size_up=int(len(train_dl)*hyper_params[\"epochs\"]*0.3), step_size_down=int(len(train_dl)*hyper_params[\"epochs\"]*0.7), mode='exp_range')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = Visualiser(\n",
    "        hyper_params[\"num_classes\"],\n",
    "        idx_to_names,\n",
    "        reverse_img_transform\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferencer = InferenceTransform(\n",
    "        idx_to_names,\n",
    "        idx_to_cls_ids,\n",
    "        hyper_params[\"regress_factor\"]\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint: temp_subset0/final.pth\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Using Multiple GPUs\n",
      "OBV-23\n",
      "https://ui.neptune.ml/gatletag/ObjectDetectionVM/e/OBV-23\n"
     ]
    }
   ],
   "source": [
    "load_components(retinanet, optimizer, scheduler, hyper_params[\"checkpoint_dir\"])\n",
    "retinanet, optimizer = amp.initialize(retinanet, optimizer, opt_level=\"O1\")\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Using Multiple GPUs\")\n",
    "    retinanet = torch.nn.DataParallel(retinanet, device_ids=range(torch.cuda.device_count())) \n",
    "retinanet = retinanet.to(hyper_params[\"device\"])\n",
    "\n",
    "cb = Callback(project_name, experiment_name, hyper_params, hyper_params[\"save_dir\"])\n",
    "\n",
    "eval_params = {\n",
    "    \"overlap\":hyper_params[\"overlap\"],\n",
    "    \"cls_thresh\":hyper_params[\"cls_thresh\"]\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "        retinanet, \n",
    "        train_dl, \n",
    "        valid_dl, \n",
    "        optimizer, \n",
    "        scheduler, \n",
    "        criterion, \n",
    "        hyper_params[\"device\"],\n",
    "        inferencer, \n",
    "        hyper_params[\"num_classes\"],\n",
    "        eval_params,\n",
    "        cb,\n",
    "        vis\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Started Epoch: 0\n",
      "0:00:20.311298 : 0 / 625\n",
      "Saving checkpoint to: temp_subset1/batch-0.pth\n",
      "0:00:25.714676 : 10 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "0:00:31.519499 : 20 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "0:00:37.078363 : 30 / 625\n",
      "0:00:42.578318 : 40 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "0:00:48.336377 : 50 / 625\n",
      "0:00:53.855667 : 60 / 625\n",
      "0:00:59.547722 : 70 / 625\n",
      "0:01:05.576483 : 80 / 625\n",
      "0:01:11.284652 : 90 / 625\n",
      "0:01:16.810005 : 100 / 625\n",
      "0:01:23.010627 : 110 / 625\n",
      "0:01:28.634089 : 120 / 625\n",
      "0:01:34.398034 : 130 / 625\n",
      "0:01:39.670341 : 140 / 625\n",
      "0:01:45.439549 : 150 / 625\n",
      "0:01:51.283162 : 160 / 625\n",
      "0:02:00.759345 : 170 / 625\n",
      "0:02:06.486337 : 180 / 625\n",
      "0:02:12.206102 : 190 / 625\n",
      "0:02:17.756951 : 200 / 625\n",
      "0:02:23.628626 : 210 / 625\n",
      "0:02:29.384763 : 220 / 625\n",
      "0:02:35.020453 : 230 / 625\n",
      "0:02:40.724088 : 240 / 625\n",
      "0:02:46.372231 : 250 / 625\n",
      "0:02:52.005651 : 260 / 625\n",
      "0:02:57.900137 : 270 / 625\n",
      "0:03:03.418728 : 280 / 625\n",
      "0:03:09.335453 : 290 / 625\n",
      "0:03:14.994342 : 300 / 625\n",
      "0:03:20.780761 : 310 / 625\n",
      "0:03:26.299806 : 320 / 625\n",
      "0:03:31.887696 : 330 / 625\n",
      "0:03:37.485367 : 340 / 625\n",
      "0:03:43.283841 : 350 / 625\n",
      "0:03:49.004330 : 360 / 625\n",
      "0:03:54.649807 : 370 / 625\n",
      "0:04:00.541219 : 380 / 625\n",
      "0:04:06.250433 : 390 / 625\n",
      "0:04:12.111985 : 400 / 625\n",
      "0:04:20.184859 : 410 / 625\n",
      "0:04:25.911408 : 420 / 625\n",
      "0:04:31.774278 : 430 / 625\n",
      "0:04:37.758742 : 440 / 625\n",
      "0:04:43.228420 : 450 / 625\n",
      "0:04:48.908036 : 460 / 625\n",
      "0:04:54.498918 : 470 / 625\n",
      "0:05:00.020727 : 480 / 625\n",
      "0:05:05.916100 : 490 / 625\n",
      "0:05:11.698752 : 500 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "0:05:16.944485 : 510 / 625\n",
      "0:05:22.657804 : 520 / 625\n",
      "0:05:28.397232 : 530 / 625\n",
      "0:05:34.266552 : 540 / 625\n",
      "0:05:39.933108 : 550 / 625\n",
      "0:05:45.761925 : 560 / 625\n",
      "0:05:51.328009 : 570 / 625\n",
      "0:05:57.117683 : 580 / 625\n",
      "0:06:02.702384 : 590 / 625\n",
      "0:06:08.378758 : 600 / 625\n",
      "0:06:14.043622 : 610 / 625\n",
      "0:06:18.420001 : 620 / 625\n",
      "Ended Train Epoch in (hours): 0:06:21.038529\n",
      "Saving checkpoint to: temp_subset1/epoch-0.pth\n",
      "[VALID] 0:00:05.518599 : 0 / 7\n",
      "Epoch completed in (hours): 0:06:35.314404\n",
      "aPs:  {'background': 0.0, 'Butterfly': 0.7514656146828416, 'Ladybug': 0.35005683406200827, 'Caterpillar': 0.35033978406474686}\n",
      "Started Epoch: 1\n",
      "0:00:11.365644 : 5 / 625\n",
      "0:00:20.985260 : 15 / 625\n",
      "0:00:26.805130 : 25 / 625\n",
      "0:00:32.730453 : 35 / 625\n",
      "0:00:38.595299 : 45 / 625\n",
      "0:00:44.316419 : 55 / 625\n",
      "0:00:49.875163 : 65 / 625\n",
      "0:00:55.668400 : 75 / 625\n",
      "0:01:01.265633 : 85 / 625\n",
      "0:01:07.085264 : 95 / 625\n",
      "0:01:12.773592 : 105 / 625\n",
      "0:01:18.194280 : 115 / 625\n",
      "0:01:24.236360 : 125 / 625\n",
      "0:01:29.879443 : 135 / 625\n",
      "0:01:35.570430 : 145 / 625\n",
      "0:01:41.408782 : 155 / 625\n",
      "0:01:47.056298 : 165 / 625\n",
      "0:01:52.780255 : 175 / 625\n",
      "0:01:58.443090 : 185 / 625\n",
      "0:02:04.189860 : 195 / 625\n",
      "0:02:09.807813 : 205 / 625\n",
      "0:02:15.509142 : 215 / 625\n",
      "0:02:21.157498 : 225 / 625\n",
      "0:02:26.999249 : 235 / 625\n",
      "0:02:32.863165 : 245 / 625\n",
      "0:02:40.997518 : 255 / 625\n",
      "0:02:46.680693 : 265 / 625\n",
      "0:02:52.648570 : 275 / 625\n",
      "0:02:58.711126 : 285 / 625\n",
      "0:03:04.606109 : 295 / 625\n",
      "0:03:10.454353 : 305 / 625\n",
      "0:03:16.052848 : 315 / 625\n",
      "0:03:21.863591 : 325 / 625\n",
      "0:03:27.684527 : 335 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "0:03:33.453202 : 345 / 625\n",
      "0:03:39.345628 : 355 / 625\n",
      "0:03:45.134989 : 365 / 625\n",
      "0:03:50.832050 : 375 / 625\n",
      "Saving checkpoint to: temp_subset1/batch-1000.pth\n",
      "0:03:56.867778 : 385 / 625\n",
      "0:04:02.389319 : 395 / 625\n",
      "0:04:08.126073 : 405 / 625\n",
      "0:04:13.806626 : 415 / 625\n",
      "0:04:19.603773 : 425 / 625\n",
      "0:04:25.169700 : 435 / 625\n",
      "0:04:31.241395 : 445 / 625\n",
      "0:04:37.098526 : 455 / 625\n",
      "0:04:42.649104 : 465 / 625\n",
      "0:04:48.387875 : 475 / 625\n",
      "0:04:54.416401 : 485 / 625\n",
      "0:05:02.516873 : 495 / 625\n",
      "0:05:08.372944 : 505 / 625\n",
      "0:05:14.277234 : 515 / 625\n",
      "0:05:20.357251 : 525 / 625\n",
      "0:05:25.952517 : 535 / 625\n",
      "0:05:31.735130 : 545 / 625\n",
      "0:05:37.372354 : 555 / 625\n",
      "0:05:43.084373 : 565 / 625\n",
      "0:05:48.788036 : 575 / 625\n",
      "0:05:54.611269 : 585 / 625\n",
      "0:06:00.292992 : 595 / 625\n",
      "0:06:06.413136 : 605 / 625\n",
      "0:06:11.341009 : 615 / 625\n",
      "Ended Train Epoch in (hours): 0:06:16.125256\n",
      "Saving checkpoint to: temp_subset1/epoch-1.pth\n",
      "[VALID] 0:00:05.558776 : 0 / 7\n",
      "Epoch completed in (hours): 0:06:24.777899\n",
      "aPs:  {'background': 0.0, 'Butterfly': 0.7328132628364408, 'Ladybug': 0.2617986513985864, 'Caterpillar': 0.3347773142869479}\n",
      "Started Epoch: 2\n",
      "0:00:08.435157 : 0 / 625\n",
      "0:00:15.060423 : 10 / 625\n",
      "0:00:20.877954 : 20 / 625\n",
      "0:00:26.476869 : 30 / 625\n",
      "0:00:32.338311 : 40 / 625\n",
      "0:00:38.144336 : 50 / 625\n",
      "0:00:43.794950 : 60 / 625\n",
      "0:00:49.842210 : 70 / 625\n",
      "0:00:55.735001 : 80 / 625\n",
      "0:01:01.305645 : 90 / 625\n",
      "0:01:10.436028 : 100 / 625\n",
      "0:01:15.919379 : 110 / 625\n",
      "0:01:21.698953 : 120 / 625\n",
      "0:01:27.537370 : 130 / 625\n",
      "0:01:33.178874 : 140 / 625\n",
      "0:01:39.059800 : 150 / 625\n",
      "0:01:44.693707 : 160 / 625\n",
      "0:01:50.608379 : 170 / 625\n",
      "0:01:56.194651 : 180 / 625\n",
      "0:02:01.884056 : 190 / 625\n",
      "0:02:07.892471 : 200 / 625\n",
      "0:02:13.613180 : 210 / 625\n",
      "0:02:19.311546 : 220 / 625\n",
      "0:02:24.995296 : 230 / 625\n",
      "0:02:30.651398 : 240 / 625\n",
      "0:02:36.268830 : 250 / 625\n",
      "0:02:42.289341 : 260 / 625\n",
      "0:02:47.786032 : 270 / 625\n",
      "0:02:53.847278 : 280 / 625\n",
      "0:02:59.290159 : 290 / 625\n",
      "0:03:04.883922 : 300 / 625\n",
      "0:03:10.908683 : 310 / 625\n",
      "0:03:16.630963 : 320 / 625\n",
      "0:03:22.515643 : 330 / 625\n",
      "0:03:28.078464 : 340 / 625\n",
      "0:03:33.716372 : 350 / 625\n",
      "0:03:42.016104 : 360 / 625\n",
      "0:03:47.702471 : 370 / 625\n",
      "0:03:53.332783 : 380 / 625\n",
      "0:03:59.117600 : 390 / 625\n",
      "0:04:04.831535 : 400 / 625\n",
      "0:04:10.684472 : 410 / 625\n",
      "0:04:16.333800 : 420 / 625\n",
      "0:04:22.114981 : 430 / 625\n",
      "0:04:27.964255 : 440 / 625\n",
      "0:04:33.700620 : 450 / 625\n",
      "0:04:39.402828 : 460 / 625\n",
      "0:04:45.247790 : 470 / 625\n",
      "0:04:50.885110 : 480 / 625\n",
      "0:04:56.798434 : 490 / 625\n",
      "0:05:02.442695 : 500 / 625\n",
      "0:05:08.194421 : 510 / 625\n",
      "0:05:14.068912 : 520 / 625\n",
      "0:05:19.523514 : 530 / 625\n",
      "0:05:25.200794 : 540 / 625\n",
      "0:05:30.870568 : 550 / 625\n",
      "0:05:36.512105 : 560 / 625\n",
      "0:05:42.347863 : 570 / 625\n",
      "0:05:48.125951 : 580 / 625\n",
      "0:05:53.844703 : 590 / 625\n",
      "0:05:59.632208 : 600 / 625\n",
      "0:06:05.420472 : 610 / 625\n",
      "0:06:09.853203 : 620 / 625\n",
      "Ended Train Epoch in (hours): 0:06:15.058724\n",
      "Saving checkpoint to: temp_subset1/epoch-2.pth\n",
      "[VALID] 0:00:05.290709 : 0 / 7\n",
      "Epoch completed in (hours): 0:06:23.486870\n",
      "aPs:  {'background': 0.0, 'Butterfly': 0.7551048938097171, 'Ladybug': 0.31024167821515714, 'Caterpillar': 0.37203085079028275}\n",
      "Started Epoch: 3\n",
      "0:00:12.769606 : 5 / 625\n",
      "0:00:18.475564 : 15 / 625\n",
      "0:00:23.860681 : 25 / 625\n",
      "0:00:30.262346 : 35 / 625\n",
      "0:00:36.209944 : 45 / 625\n",
      "0:00:41.904462 : 55 / 625\n",
      "0:00:47.785503 : 65 / 625\n",
      "0:00:53.600410 : 75 / 625\n",
      "0:00:59.262442 : 85 / 625\n",
      "0:01:04.941645 : 95 / 625\n",
      "0:01:10.712514 : 105 / 625\n",
      "0:01:16.306435 : 115 / 625\n",
      "0:01:22.155020 : 125 / 625\n",
      "Saving checkpoint to: temp_subset1/batch-2000.pth\n",
      "0:01:28.228756 : 135 / 625\n",
      "0:01:34.017561 : 145 / 625\n",
      "0:01:39.786687 : 155 / 625\n",
      "0:01:45.437865 : 165 / 625\n",
      "0:01:51.160149 : 175 / 625\n",
      "0:01:57.005442 : 185 / 625\n",
      "0:02:02.974322 : 195 / 625\n",
      "0:02:08.788516 : 205 / 625\n",
      "0:02:17.913754 : 215 / 625\n",
      "0:02:23.673390 : 225 / 625\n",
      "0:02:29.339700 : 235 / 625\n",
      "0:02:35.079989 : 245 / 625\n",
      "0:02:40.879276 : 255 / 625\n",
      "0:02:46.855228 : 265 / 625\n",
      "0:02:52.546151 : 275 / 625\n",
      "0:02:58.304055 : 285 / 625\n",
      "0:03:04.324502 : 295 / 625\n",
      "0:03:10.296756 : 305 / 625\n",
      "0:03:16.023349 : 315 / 625\n",
      "0:03:22.022675 : 325 / 625\n",
      "0:03:28.012104 : 335 / 625\n",
      "0:03:33.496844 : 345 / 625\n",
      "0:03:39.453611 : 355 / 625\n",
      "0:03:45.313318 : 365 / 625\n",
      "0:03:51.041296 : 375 / 625\n",
      "0:03:56.643108 : 385 / 625\n",
      "0:04:02.373845 : 395 / 625\n",
      "0:04:08.133241 : 405 / 625\n",
      "0:04:14.214940 : 415 / 625\n",
      "0:04:20.002300 : 425 / 625\n",
      "0:04:25.711598 : 435 / 625\n",
      "0:04:31.728091 : 445 / 625\n",
      "0:04:39.814947 : 455 / 625\n",
      "0:04:45.642284 : 465 / 625\n",
      "0:04:51.401491 : 475 / 625\n",
      "0:04:57.362797 : 485 / 625\n",
      "0:05:02.858144 : 495 / 625\n",
      "0:05:08.789190 : 505 / 625\n",
      "0:05:14.608173 : 515 / 625\n",
      "0:05:20.267079 : 525 / 625\n",
      "0:05:25.940437 : 535 / 625\n",
      "0:05:31.909183 : 545 / 625\n",
      "0:05:37.510101 : 555 / 625\n",
      "0:05:43.168627 : 565 / 625\n",
      "0:05:48.874801 : 575 / 625\n",
      "0:05:54.628483 : 585 / 625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:00.318225 : 595 / 625\n",
      "0:06:06.058796 : 605 / 625\n",
      "0:06:11.238306 : 615 / 625\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "Ended Train Epoch in (hours): 0:06:16.007337\n",
      "Saving checkpoint to: temp_subset1/epoch-3.pth\n",
      "[VALID] 0:00:05.667291 : 0 / 7\n",
      "Epoch completed in (hours): 0:06:24.793316\n",
      "aPs:  {'background': 0.0, 'Butterfly': 0.7942650082427809, 'Ladybug': 0.2976648106580695, 'Caterpillar': 0.3996718386768343}\n",
      "Started Epoch: 4\n",
      "0:00:07.892249 : 0 / 625\n",
      "0:00:14.791538 : 10 / 625\n",
      "0:00:20.636052 : 20 / 625\n",
      "0:00:26.272409 : 30 / 625\n",
      "0:00:32.201145 : 40 / 625\n",
      "0:00:41.393283 : 50 / 625\n",
      "0:00:46.839903 : 60 / 625\n",
      "0:00:52.461734 : 70 / 625\n",
      "0:00:58.234641 : 80 / 625\n",
      "0:01:04.140579 : 90 / 625\n",
      "0:01:09.973421 : 100 / 625\n",
      "0:01:15.837182 : 110 / 625\n",
      "0:01:21.433337 : 120 / 625\n",
      "0:01:27.097383 : 130 / 625\n",
      "0:01:32.953521 : 140 / 625\n",
      "0:01:38.812258 : 150 / 625\n",
      "0:01:44.306968 : 160 / 625\n",
      "0:01:50.135553 : 170 / 625\n",
      "0:01:55.994042 : 180 / 625\n",
      "0:02:01.802348 : 190 / 625\n",
      "0:02:07.553381 : 200 / 625\n",
      "0:02:13.237517 : 210 / 625\n",
      "0:02:18.927855 : 220 / 625\n",
      "0:02:24.701405 : 230 / 625\n",
      "0:02:30.547138 : 240 / 625\n",
      "0:02:36.417518 : 250 / 625\n",
      "0:02:44.517613 : 260 / 625\n",
      "0:02:50.339839 : 270 / 625\n",
      "0:02:55.904568 : 280 / 625\n",
      "0:03:01.643382 : 290 / 625\n",
      "0:03:07.336788 : 300 / 625\n",
      "0:03:13.291947 : 310 / 625\n",
      "0:03:18.872495 : 320 / 625\n",
      "0:03:24.633421 : 330 / 625\n",
      "0:03:30.321235 : 340 / 625\n",
      "0:03:35.982760 : 350 / 625\n",
      "0:03:41.978113 : 360 / 625\n",
      "0:03:47.633057 : 370 / 625\n",
      "0:03:53.074650 : 380 / 625\n",
      "0:03:58.801370 : 390 / 625\n",
      "0:04:04.513336 : 400 / 625\n",
      "0:04:10.554532 : 410 / 625\n",
      "0:04:16.269302 : 420 / 625\n",
      "0:04:22.067379 : 430 / 625\n",
      "0:04:28.009310 : 440 / 625\n",
      "0:04:33.711106 : 450 / 625\n",
      "0:04:39.513944 : 460 / 625\n",
      "0:04:45.316121 : 470 / 625\n",
      "0:04:51.243100 : 480 / 625\n",
      "0:04:56.926479 : 490 / 625\n",
      "0:05:04.905755 : 500 / 625\n",
      "Saving checkpoint to: temp_subset1/batch-3000.pth\n",
      "0:05:11.154274 : 510 / 625\n",
      "0:05:16.816382 : 520 / 625\n",
      "0:05:22.595977 : 530 / 625\n",
      "0:05:28.393542 : 540 / 625\n",
      "0:05:34.443347 : 550 / 625\n",
      "0:05:39.801806 : 560 / 625\n",
      "0:05:46.058964 : 570 / 625\n",
      "0:05:51.894972 : 580 / 625\n",
      "0:05:57.545237 : 590 / 625\n",
      "0:06:03.388423 : 600 / 625\n",
      "0:06:09.054957 : 610 / 625\n",
      "0:06:13.350586 : 620 / 625\n",
      "Ended Train Epoch in (hours): 0:06:15.979810\n",
      "Saving checkpoint to: temp_subset1/epoch-4.pth\n",
      "[VALID] 0:00:05.577580 : 0 / 7\n",
      "Epoch completed in (hours): 0:06:24.694664\n",
      "aPs:  {'background': 0.0, 'Butterfly': 0.8029118536487827, 'Ladybug': 0.318639241402348, 'Caterpillar': 0.4250957215295883}\n",
      "Saving checkpoint to: temp_subset1/final.pth\n",
      "Training completed in (hours): 0:32:16.483864\n"
     ]
    }
   ],
   "source": [
    "trainer.train(hyper_params[\"epochs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
